{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mutual_info_score, accuracy_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/housing.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values\n",
      " total_bedrooms    157\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Keep only  '<1H OCEAN' and 'INLAND' from ocean_proximity\n",
    "df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n",
    "\n",
    "# Fill missing values\n",
    "print('Missing values\\n',df.isnull().sum()[4:5])\n",
    "df.fillna(0, inplace = True) \n",
    "\n",
    "# Log transform to median_house_value\n",
    "df['median_house_value'] = df['median_house_value'].agg(np.log1p)\n",
    "\n",
    "# Train/Validation/Test split\n",
    "df_train_large, df_test = train_test_split(df, test_size = 0.2, random_state = 1)\n",
    "df_train, df_val = train_test_split(df_train_large, train_size = 0.75, random_state = 1)\n",
    "\n",
    "# Convert DataFrames to dictionary records\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "train_large_dict = df_train_large.to_dict(orient='records')\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "test_dict = df_test.to_dict(orient='records')\n",
    "\n",
    "dv = DictVectorizer(sparse=True)\n",
    "\n",
    "# Fit and transform\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "X_train_large = dv.transform(train_large_dict)\n",
    "X_val = dv.transform(val_dict)\n",
    "X_test = dv.transform(test_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a hypothetical dataset with multiples numerical features $\\mathbf{X}_{1},\\cdots, \\mathbf{X}_{d}$ and a target variable $\\mathbf{Y}$. These features and the target variable are represented as column vectors in the dataset, as shown:\n",
    "\n",
    "$$\n",
    "\\left( \\begin{array}{c|ccc|c}\n",
    "\\text{Instance}    &\\mathbf{X}_{1}&\\cdots & \\mathbf{X}_{d}  & \\mathbf{Y}\\\\\n",
    "\\hline\n",
    "\\mathbf{x}_{1} & x_{11}& \\cdots&x_{1d}&y_1 \\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{x}_{n}&x_{n1}&\\cdots&x_{nd}&y_n\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "In this representation, each row vector $ \\mathbf{x}_i = ( x_{i1}, \\ldots, x_{id})$ constitutes an instance of the dataset, with $d$ feature values. The index $i$ ranges from 1 to $n$, where $n$ is the total number of instances. The dataset can be further decomposed into a feature matrix $\\mathbf{X}$ and a target vector $\\mathbf{Y}$:\n",
    "\n",
    "$$\\mathbf{X}=\n",
    "\\left( \\begin{array}{ccc}\n",
    "  x_{11}& \\cdots&x_{1d} \\\\\n",
    "\\vdots&\\ddots&\\vdots&\\\\\n",
    "x_{n1}&\\cdots&x_{nd}\n",
    "\\end{array} \\right) ~~~ \\text{and} ~~~ \n",
    "\n",
    "\\mathbf{Y} = \\left( \\begin{array}{c}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Here, the feature matrix $\\mathbf{X}$ is composed of all feature vectors, while the target vector $\\mathbf{Y}$ takes discrete values, which could be binary or multi-classes. Each unique class in $\\mathcal{Y}$ corresponds to a partition of the dataset, formed by grouping instances $\\mathbf{x}_i$ with the same class.\n",
    "\n",
    "\n",
    "A decision tree is a recursive model that employs partition-based methods to predict the class $\\hat{y}_i$ for each instance $\\mathbf{x}_i$. The process starts by splitting the dataset into two partitions. These partitions are further divided recursively, until a state is achieved in which the majority of instances  $\\mathbf{x}_i$ within a partition belong to the same class.\n",
    "\n",
    "One important partition-based method employed in most decision tree models, such as CART, is the axis-parallel hyperplane. This approach is commonly used in high-dimensional spaces represented by the dataset's features. The term \"hyperplane\" refers to the generalization of a geometric plane in a space with more than three dimensions.\n",
    "\n",
    "The mathematical formulation for such a hyperplane is given by the condition:\n",
    "\n",
    "$$h(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b\\leq 0$$\n",
    "\n",
    "Here, the $\\mathbf{x}$ variable in this function $h(\\mathbf{x})$ can be any instance from the dataset. The weight vector $\\mathbf{w}$ is restricted a priori to one of the standard basis vectors $\\{\\mathbf{e}_1,\\cdots,\\mathbf{e}_j,\\cdots \\mathbf{e}_d\\}$, where $\\mathbf{e}_j$ has a value of 1 for the jth dimension and 0 for all other dimensions. This implies that the weights determine the orientation of the hyperplane in one of the basis vector directions, while the bias term translates it along that axis. The base vectors of a $d$-dimensional space are given by:\n",
    "\n",
    "$$ \n",
    "\\mathbf{e}_1 = \\left( \\begin{array}{c}\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array} \\right),~\n",
    "\\mathbf{e}_i = \\left( \\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array} \\right),~~\n",
    "\\mathbf{e}_d = \\left( \\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "On the other hand, the inequality $h(\\mathbf{x}) \\leq 0$ serves a particular purpose: it defines a half-space. Any instance $\\mathbf{x}$ for which $h(\\mathbf{x}) \\leq 0$ lies on one side of the hyperplane, and any instance for which $h(\\mathbf{x}) > 0$ lies on the other side. In this way, the hyperplane acts as a decision boundary that partitions the dataset into two partitions based on the sign of $h(\\mathbf{x})$.\n",
    "\n",
    "For a given standard basis vector chosen as the weight $\\mathbf{w} = \\mathbf{e}_j$, where $j$ can be an integer between 1 and $d$, the decision condition $h(\\mathbf{x})$ for some instance $\\mathbf{x}_i$ is represented by:\n",
    "\n",
    "$$h(\\mathbf{x}_i) = \\mathbf{e}_j \\cdot \\mathbf{x}_i  + b \\leq 0$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$x_{ij} \\leq t$$\n",
    "\n",
    "Here $t = -b$ represents a specific value within the domain of the feature vector $\\mathbf{X}_j$.The split condition for the ith feature will be then the value of the jth element from the row vector $\\mathbf{X}_j$. The optimal offset $b$ is chosen to minimize a particular criterion, such as a loss function, for the partitioned datasets.\n",
    "\n",
    "Upon applying the decision boundary, the dataset $\\mathcal{D}$ is divided into two mutually exclusive partitions: $\\mathcal{D}_Y$ and $\\mathcal{D}_N$. The partition $\\mathcal{D}_Y$ includes instances that satisfy the inequality $x_{ij} \\leq t$, while $\\mathcal{D}_N$ includes those that do not. More formally, for each instance $\\mathbf{x}_i$:\n",
    "\n",
    "- If the $j$-th feature $x_{ij}$ meets the condition $x_{ij} \\leq t$, then the instance is allocated to the set\n",
    "\n",
    "  $$\\mathcal{D}_Y = \\{\\mathbf{x}_i| x_{ij} \\leq  t\\}$$\n",
    "  \n",
    "  which grouped all instances $\\mathbf{x}_i$ such that their $j$-th feature $x_{ij}$ is less than or equal to the threshold $t$.\n",
    "\n",
    "- Otherwise, the instance are allocated into the set\n",
    "\n",
    "  $$\\mathcal{D}_N = \\{\\mathbf{x}_i| x_{ij} >  t\\ \\}$$\n",
    "\n",
    "In this context, $t$ is a pre-selected threshold that serves as the decision boundary that separates the two partitions\n",
    "\n",
    "\n",
    "Given that $x_{ij}$ is a element from the feature vector $\\mathbf{X}_j$, we can turn this more explicitly in the notation. To express this condition in a more generic form that represents the behavior across the entire feature, we can write the inequality $x_{ij} \\leq t$ for all $i = 1, 2, \\ldots, n$ as\n",
    "\n",
    "$$\\mathbf{X}_j \\leq t$$\n",
    "\n",
    "This generic form implicitly implies that the condition is applicable to each element $x_{ij}$, where $i = 1, 2, \\ldots, n $ across the entire dataset $\\mathcal{D}$. By using this generic representation, the rules becomes a general principle that applies not just to individual instances, but to the entire feature, providing a view on how the feature $\\mathbf{X}_j$ contributes to the partitioning of $\\mathcal{D}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Particular case in two dimension**\n",
    "\n",
    "To illuminate the partition-based methods used in the decision tree model, let's consider a hypothetical scenario involving a synthetically generated dataset. This dataset will have two features, $\\mathbf{X}_1$ and $\\mathbf{X}_2$, and a multi-class target variable $\\mathbf{Y}$ with four possible classes. Mathematically the dataset can be represented as:\n",
    "\n",
    "$$\n",
    "\\left( \\begin{array}{c|cc|c}\n",
    "\\text{Instance}    &\\mathbf{X}_{1}& \\mathbf{X}_{2}  & \\mathbf{Y}\\\\\n",
    "\\hline\n",
    "\\mathbf{x}_{1} & x_{11}&x_{12}&y_1 \\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{x}_{n}&x_{n1}&x_{n2}&y_n\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "where each instance will be randomly generated to be in one of the four possible classes of the feature vector $\\mathbf{Y}$. Then two threshold values are selected to partition the dataset  $\\mathcal{D}$ into four distinct regions $\\mathcal{D}_1$, $\\mathcal{D}_2$  $\\mathcal{D}_3$ and $\\mathcal{D}_4$ .\n",
    "\n",
    "<center><img src = \"Images/axis_parallel.png\" width=\"600\" height=\"500\"/></center>\n",
    "\n",
    "By observing the plot, we can gain a visual intuition into how partition-based methods would partition the feature space to isolate instances of different classes.  Each region within this space is defined by a set of rules. These rules act as decision conditions, determining the region to which each instance belongs. When the model evaluates a given instance, it follows this set of decision rules:\n",
    "\n",
    "\n",
    "- $\\mathcal{D_1}$: If $\\mathbf{X}_1 \\leq 3.5$ and $\\mathbf{X}_2 > 4.5$, then belongs to  class 4\n",
    "- $\\mathcal{D_2}$: If $\\mathbf{X}_1 > 3.5$ and $\\mathbf{X}_2 > 4.5$, then belongs to  class 2\n",
    "- $\\mathcal{D}_3$: If $\\mathbf{X}_1 \\leq 3.5$ and $\\mathbf{X}_2 \\leq 4.5$, then belongs to  class 1\n",
    "- $\\mathcal{D_4}$: If $\\mathbf{X}_1 > 3.5$ and $\\mathbf{X}_2 \\leq 4.5$, then belongs to  class 3\n",
    "\n",
    "After established the partitioning of our feature space into distinct regions, we can represent these partitions in a more structured form, using decision tree diagram, which gives the name of the model. A decision tree diagram will represent a series of decisions made on the features of the dataset to reach a conclusion about the class of an instance. \n",
    "\n",
    "To those decision rules, we can draw the following diagram which visually can give a interpretation of the decision-making process in classifying an instance based on its feature values. Consider the following diagram of a decision tree:\n",
    "\n",
    "<center><img src = \"Images/decision_tree_diagram.png\" width=\"600\" height=\"400\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating split points $t$**\n",
    "\n",
    "For a numerical or categorical attribute split point, defined as $X\\leq v$ or $X \\in \\mathbf{V}$, we require a scoring criterion to effectively separate the different class labels ${c_1, \\cdots, c_k}$.\n",
    "\n",
    "**Information Gain**\n",
    "\n",
    "Information gain measures the reduction of disorder or uncertainty in a system. The goal is to use entropy as a metric for each partition, favoring a lower entropy if the partition is pure (i.e., most points have the same class label). Conversely, if class labels are mixed with no majority class, a partition has higher entropy.\n",
    "\n",
    "The entropy of a set of labeled points $\\mathbf{D}$ is defined as:\n",
    "\n",
    "$$H(\\mathbf{D}) = - \\sum^{k}_{i=1} P(c_i| \\mathbf{D}) \\log{P(c_i| \\mathbf{D})}$$\n",
    "\n",
    "where $P(x_i| \\mathbf{D})$ is the probability of class $c_i$ in $\\mathbf{D}$, and $k$ is the number of classes.\n",
    "\n",
    "When a split point partitions $\\mathbf{D}$ into $\\mathbf{D}_Y$ and $\\mathbf{D}_N$, we define the split entropy as the weighted entropy of each of the resulting partitions:\n",
    "\n",
    "$$H(\\mathbf{D}_Y, \\mathbf{D}_N) = \\frac{n_Y}{n}H(\\mathbf{D}_Y) + \\frac{n_N}{n}H(\\mathbf{D}_N)$$\n",
    "\n",
    "where $n = |\\mathbf{D}|$ is the number of points in $\\mathbf{D}$, and $n_Y$ and $n_N$ are the number of points in $\\mathbf{D}_Y$ and $\\mathbf{D}_N$, respectively.\n",
    "\n",
    "The information gain for a given split point, representing the reduction in overall entropy, is defined as:\n",
    "\n",
    "$$\\text{Grain}(\\mathbf{D},\\mathbf{D}_Y, \\mathbf{D}_N) = H(\\mathbf{D}) - H(\\mathbf{D}_Y, \\mathbf{D}_N)$$\n",
    "\n",
    "A higher information gain corresponds to a greater reduction in entropy, thus signaling a better split point. Therefore, we can score each split point and select the one that provides the highest information gain.\n",
    "\n",
    "\n",
    "**Gini Index**\n",
    "\n",
    "The Gini index, another common purity measure for a split point, is defined as:\n",
    "\n",
    "$$G(\\mathbf{D}) = 1 - \\sum_{i=1}^{k} P(c_i| \\mathbf{D})^2$$\n",
    "\n",
    "Higher Gini index values signify more disorder, while lower values indicate a higher order with respect to the class labels. The weighted Gini index of a split point is then defined as:\n",
    "\n",
    "$$G(\\mathbf{D}_Y, \\mathbf{D}_N) = \\frac{n_Y}{n}G(\\mathbf{D}_Y) + \\frac{n_N}{n}G(\\mathbf{D}_N)$$\n",
    "\n",
    "\n",
    "**CART**\n",
    "\n",
    "Another useful measure is the CART, defined as:\n",
    "\n",
    "$$\\text{CART}(\\mathbf{D}_Y, \\mathbf{D}_N) = 2 \\frac{n_Y}{n}\\frac{n_N}{n} \\sum_{i = 1}^k |P(c_i| \\mathbf{D}_Y )  -P(c_i| \\mathbf{D}_N)$$\n",
    "\n",
    "This metric maximizes the difference between the class probability mass function for the two partitions; a higher CART value implies a better split point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left( \\begin{array}{c|cc|c}\n",
    "\\text{Instance}    &\\mathbf{X}_{1}& \\mathbf{X}_{2}  & \\mathbf{Y}\\\\\n",
    "\\hline\n",
    "\\mathbf{x}_{1} & x_{11}&x_{12}&y_1 \\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{x}_{n}&x_{n1}&x_{n2}&y_n\n",
    "\\end{array} \\right).$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
