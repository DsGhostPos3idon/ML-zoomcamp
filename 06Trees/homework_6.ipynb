{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mutual_info_score, accuracy_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/housing.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values\n",
      " total_bedrooms    157\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Keep only  '<1H OCEAN' and 'INLAND' from ocean_proximity\n",
    "df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n",
    "\n",
    "# Fill missing values\n",
    "print('Missing values\\n',df.isnull().sum()[4:5])\n",
    "df.fillna(0, inplace = True) \n",
    "\n",
    "# Log transform to median_house_value\n",
    "df['median_house_value'] = df['median_house_value'].agg(np.log1p)\n",
    "\n",
    "# Train/Validation/Test split\n",
    "df_train_large, df_test = train_test_split(df, test_size = 0.2, random_state = 1)\n",
    "df_train, df_val = train_test_split(df_train_large, train_size = 0.75, random_state = 1)\n",
    "\n",
    "# Convert DataFrames to dictionary records\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "train_large_dict = df_train_large.to_dict(orient='records')\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "test_dict = df_test.to_dict(orient='records')\n",
    "\n",
    "dv = DictVectorizer(sparse=True)\n",
    "\n",
    "# Fit and transform\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "X_train_large = dv.transform(train_large_dict)\n",
    "X_val = dv.transform(val_dict)\n",
    "X_test = dv.transform(test_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a hypothetical dataset with multiple features $\\mathbf{X}_{1},\\cdots, \\mathbf{X}_{d}$ and a target variable $\\mathbf{Y}$ as shown:\n",
    "\n",
    "$$\n",
    "\\left( \\begin{array}{c|ccc|c}\n",
    "\\text{Instance}    &\\mathbf{X}_{1}&\\cdots & \\mathbf{X}_{d}  & \\mathbf{Y}\\\\\n",
    "\\hline\n",
    "\\mathbf{x}_{1} & x_{11}& \\cdots&x_{1d}&y_1 \\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{x}_{n}&x_{n1}&\\cdots&x_{nd}&y_n\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "Here, each row vector is an instance $ \\mathbf{x}_i = ( x_{i1}, \\ldots, x_{id}) $ of the dataset with $ d$ values. The dataset is separated in a feature matrix $\\mathbf{X}$ and a target vector $\\mathbf{Y}$:\n",
    "\n",
    "$$\\mathbf{X}=\n",
    "\\left( \\begin{array}{ccc}\n",
    "  x_{11}& \\cdots&x_{1d} \\\\\n",
    "\\vdots&\\ddots&\\vdots&\\\\\n",
    "x_{n1}&\\cdots&x_{nd}\n",
    "\\end{array} \\right) ~~~ \\text{and} ~~~ \n",
    "\n",
    "\\mathbf{Y} = \\left( \\begin{array}{c}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "The target vector $\\mathbf{Y}$ consists of non-binary values. Each element, $ y_i $, in $\\mathbf{Y}$ stands for one of the potential $k$ class labels, which is captured by the set $ \\mathcal{Y} = \\{c_1, c_2, \\ldots, c_k\\} $. These unique classes in $\\mathcal{Y}$ have corresponding partition of the dataset, by grouping instances $\\mathbf{x}_i$ by their class label $c_j$. \n",
    "\n",
    "A decision tree is a recursive model that employs partition-based methods to predict the class $\\hat{y}_i$ for each instance $\\mathbf{x}_i$. The process starts by splitting the dataset into two partitions. These partitions are further divided recursively, until achieve a state where the majority of instances $\\mathbf{x}_i$ within a partition belong to the same class label $c_j$.\n",
    "\n",
    "One important partition-based method employed in most decision tree model, like CART, is the axis-parallel hyperplane. This approach is commonly used in high-dimensional spaces represented by the dataset's features. The term \"hyperplane\" refers to the generalization of a geometric plane in a space with more than three dimensions.\n",
    "\n",
    "The mathematical formulation for such a hyperplane is given by the condition:\n",
    "\n",
    "$$h(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b\\leq 0$$\n",
    "\n",
    "Here, $\\mathbf{x}$ can be any instance from the dataset. The weight vector $\\mathbf{w}$ is restricted a priori to one of the standard basis vectors $\\{\\mathbf{e}_1,\\cdots,\\mathbf{e}_j,\\cdots \\mathbf{e}_d\\}$, where $\\mathbf{e}_j$ has a value of 1 for the jth dimension and 0 for all other dimensions. This implies that the weights determine the orientation of the hyperplane in one of the basis vector directions, while the bias term translates it along that axis. The base vectors of a $d$-dimensional space are given by:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\mathbf{e}_1 = \\left( \\begin{array}{c}\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array} \\right),~\n",
    "\\mathbf{e}_i = \\left( \\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array} \\right),~~\n",
    "\\mathbf{e}_d = \\left( \\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "On the other hand, the inequality $h(\\mathbf{x}) \\leq 0$ serves a particular purpose: it defines a half-space. Any instance $\\mathbf{x}$ for which $h(\\mathbf{x}) \\leq 0$ lies on one side of the hyperplane, and any instance for which $h(\\mathbf{x}) > 0$ lies on the other side. In this way, the hyperplane acts as a decision boundary that partitions the dataset into two partitions based on the sign of $h(\\mathbf{x})$.\n",
    "\n",
    "For a given standard basis vector chosen as the weight $\\mathbf{w} = \\mathbf{e}_j$, the decision condition $h(\\mathbf{x})$ for some instance $\\mathbf{x}_i$ is represented by:\n",
    "\n",
    "$$h(\\mathbf{x}_i) = \\mathbf{e}_j \\cdot \\mathbf{x}_i  + b \\leq 0$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$x_{ij} \\leq v$$\n",
    "\n",
    "Where $v = -b$ is a specific value within the domain of the feature vector $\\mathbf{X}_i$.The split condition for the ith feature will be then the value of the jth element from the row vector $\\mathbf{X}_i$. The optimal offset $b$ is chosen to minimize a particular criterion, such as a loss function, for the partitioned datasets.\n",
    "\n",
    "Upon applying this decision boundary, the dataset $\\mathcal{D}$ is split into two distinct partitions: $\\mathcal{D}_Y$ and $\\mathcal{D}_N$. In this partitioning, $\\mathcal{D}_Y$ consists of those instances that satisfy the decision boundary $x_{ij} \\leq v$, while $\\mathcal{D}_N$ comprises those that do not satisfy the decision boundary. Thus, for each instance $\\mathbf{x}_i$:\n",
    "\n",
    "If it meets the condition $\\mathbf{x}_i \\leq  v$\n",
    "\n",
    "$$\\mathcal{D}_Y = \\{\\mathbf{x}_i| x_{ij} \\leq  v\\}$$\n",
    "\n",
    "otherwise,\n",
    "\n",
    "$$\\mathcal{D}_N = \\{\\mathbf{x}_i| x_{ij} >  v\\ \\}$$\n",
    "\n",
    "Here, $v$ is a chosen threshold that delineates the two partitions. All instances $\\mathbf{x}_i $ for which the $j$-th feature $x_{ij} $ is less than or equal to $v $ will be allocated to the partition $\\mathcal{D}_Y$. In contrast, those for which $x_{ij} > v $ will be grouped into $\\mathcal{D}_N$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
