{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from scipy.signal import convolve\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "#from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# **Convolutional Neural Networks (CNNs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is essentially a feedforward Multi-Layer Perceptron (MLP) that is designed to recognize local patterns and sparsity in input data. Like the MLP, each neuron is connected to others through learnable weights. These weights are adjusted during training to optimize the network's performance for a specific task.\n",
    "\n",
    "The main difference between MLPs and CNNs is that the latter is developed for processing multidimensional data, such as images or videos. Also, CNNs have a more diverse set of specialized layers, including convolutional layers, pooling layers, and upsampling layers, which are optimized for processing spatial (image) and temporal data (video)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convolutions**\n",
    "\n",
    "Convolution is a mathematical operation that involves sliding a small matrix, known as a kernel or filter, across a larger matrix representing the input data, such as an image. During this process, the element-wise product $\\odot$ is computed between the kernel and each local region (sub-matrix) it covers on the input data matrix. The result of this operation is a new matrix, called a feature map, which encodes information about the presence, absence, or strength of specific features in the input data.\n",
    "\n",
    "Let's examine the following convolutional operations to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One Dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider $\\mathbf{x}$ as an input vector with $n$ elements and $\\mathbf{w}$ as a weight vector, also known as a  **filter**, with $k \\leq n$.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\left( \\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}\n",
    "\\end{array} \\right), ~~~~\n",
    "\\mathbf{w} =\n",
    "\\left( \\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "\\vdots\\\\\n",
    "w_{k}\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Here $k$  is known as the  **window size** and indicates the size of the filter applied to the input vector $\\mathbf{x}$. It defines the region of the local neighborhood within the input vector $\\mathbf{x}$ used for computing output values. To proceed, we define a subvector of $\\mathbf{x}$ with the same size as the filter vector. Let $\\mathbf{x}_k(i)$ denote the window of $\\mathbf{x}$  of size $k$ starting at position $i$:\n",
    "\n",
    "$$\\mathbf{x}_k(i) = \\left( \\begin{array}{c}\n",
    "x_i \\\\\n",
    "x_{i+1} \\\\\n",
    "\\vdots\\\\\n",
    "x_{i+k-1} \n",
    "\\end{array} \\right).$$\n",
    "\n",
    "For $k \\leq n$, it must be that $i+k-1 \\leq n$, implying $1 \\leq i \\leq n-k+1$. As a validity test, if we start at $i =  n-k+1$, then the end position is $i+k-1 = n$. If we calculate the total number of elements by the difference in position provides the window size $k$, confirmed by $n - i = n - (n-k+1) = k$. For example, with $n = 5$ and $k = 3$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\left( \\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "x_{3}\\\\\n",
    "x_{4}\\\\\n",
    "x_{5}\n",
    "\\end{array} \\right), ~~~~\n",
    "\\mathbf{w} =\n",
    "\\left( \\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "w_{3}\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "the window of $\\mathbf{x}$ from $i = 2$ to $i+k-1 = 4$ is:\n",
    "\n",
    "$$\\mathbf{x}_3(2) = \\left( \\begin{array}{c}\n",
    "x_2 \\\\\n",
    "x_{3}\\\\\n",
    "x_{4}\n",
    "\\end{array} \\right)$$\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's first consider a particular example with input vector $\\mathbf{x}$ of size $n = 5$ and a weight vector with window size $k = 3$. The vectors are illustrated in the following figure:\n",
    "\n",
    "<center><img src = \"figures/1d-conv.png\" width=\"800\" height=\"400\"/></center>\n",
    "\n",
    "\n",
    "The convolution steps for the sliding windows of $\\mathbf{x}$ with the filter $\\mathbf{w}$ are:\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum \\mathbf{x}_3(1) \\odot \\mathbf{w} = \\sum (1, 3, -1)^T \\odot (1, 0, 2)^T = \\sum  (1 \\cdot 1, 3 \\cdot 0, -1 \\cdot 2) = 1 + 0 - 2 = -1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum \\mathbf{x}_3(2) \\odot \\mathbf{w} = \\sum (3, -1, 2)^T \\odot (1, 0, 2)^T = \\sum  (3 \\cdot 1, -1 \\cdot 0, 2 \\cdot 2) = 3 + 0 + 4 = 7,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum \\mathbf{x}_3(3) \\odot \\mathbf{w} = \\sum (-1, 2, 3)^T \\odot (1, 0, 2)^T = \\sum  (-1 \\cdot 1, 2 \\cdot 0, 3 \\cdot 2) = -1 + 0 + 6 = 5.\n",
    "$$\n",
    "\n",
    "The element-wise product $\\odot$  , also known as the Hadamard product, multiplies corresponding elements in two vectors. Unlike the typical inner product, which multiplies an element by a column, this operation multiplies an element by its corresponding element in another vector. This steps provide the convolution between the two vectors resulting in a vector of size n-k+1 = 3. Thus, the convolution $\\mathbf{x} * \\mathbf{w}$ is:\n",
    "\n",
    "$$\\mathbf{x} * \\mathbf{w} =\n",
    "\\left( \\begin{array}{c}\n",
    "-1\\\\\n",
    "7\\\\\n",
    "5\n",
    "\\end{array} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector X: (5,)\n",
      "filter W: (3,)\n",
      "output X*W: [-1  7  5]\n"
     ]
    }
   ],
   "source": [
    "# Code for the example\n",
    "\n",
    "X = np.array([1, 3, -1, 2, 3])\n",
    "# flip the filter W to use the convolve function\n",
    "# as expected in machine learning and deep learning context\n",
    "W = np.flip(np.array([1, 0, 2]))\n",
    "\n",
    "# perform 1D convolution\n",
    "output = np.convolve(X, W, mode='valid')\n",
    "\n",
    "print(\"Input vector X:\", X.shape)\n",
    "print(\"filter W:\", W.shape)\n",
    "print(\"output X*W:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates that convolution is an element-wise product between a subvector and a weight vector of the same size, providing a scalar value when summed, which forms the result of the convolution operation.\n",
    "\n",
    "To simplify the notation, let's adopt the convention that for a vector $\\mathbf{a} \\in \\mathbb{R}^k$, define the summation operator as one that adds all elements of the vector. That is, \n",
    "\n",
    "$$\\text{Sum}(\\mathbf{a}) = \\sum_{i=1}^{k} a_{i}$$ \n",
    "\n",
    "Then from the example, we would have\n",
    "\n",
    "$$\n",
    "\\sum \\mathbf{x}_3(1) \\odot \\mathbf{w} = \\text{Sum}\\bigg( \\mathbf{x}_3(1) \\odot \\mathbf{w} \\bigg)= 1 + 0 - 2 = -1.\n",
    "$$\n",
    "\n",
    "Then, we can define a general one dimensional convolution operation between $\\mathbf{x}$ and $\\mathbf{w}$, denoted by the asterisk symbol $\\ast$, as \n",
    "\n",
    "$$\\mathbf{x} \\ast \\mathbf{w} = \\left( \\begin{array}{c}\n",
    "\\text{Sum}(\\mathbf{x}_k(1) \\odot \\mathbf{w})\\\\\n",
    "\\vdots\\\\\n",
    "\\text{Sum}(\\mathbf{x}_k(i) \\odot \\mathbf{w})\\\\\n",
    "\\vdots\\\\\n",
    "\\text{Sum}(\\mathbf{x}_k(n-k+1) \\odot \\mathbf{w})\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "The convolution of $\\mathbf{x} \\in \\mathbf{R}^{n}$ and $\\mathbf{W} \\in \\mathbf{R}^{k}$ results in a vector of size $n-k+1$. The i-th element from this output vector can be decomposed as\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{x_k}(i) \\odot \\mathbf{w}) = x_{i}w_1 + x_{i+1}w_2 + \\cdots + x_{(i+k-1)}w_k =  \\sum_{j=1}^{k} x_{(i+j-1)}w_j.$$\n",
    "\n",
    "This shows that the sum is over all elements of the subvector $\\mathbf{x}_k(i)$, so the last element of this sum must coincide with the last elements of $\\mathbf{x}_k(i)$ and $\\mathbf{w}$. This results in the convolution of $\\mathbf{x}$ with $\\mathbf{w}$ over the window defined by $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Two Dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the convolution operation to an matrix input instead of a vector. Let $\\mathbf{X}$ be an input matrix with $n \\times n$ elements and $\\mathbf{W}$ be the weight matrix, also known as a  **filter**, with $k \\leq n$.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n",
    "x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,n}\n",
    "\\end{bmatrix},~~\n",
    "\\mathbf{W}=\\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\cdots & w_{1,k} \\\\\n",
    "w_{2,1} & w_{2,2} & \\cdots & w_{2,k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{k,1} & w_{k,2} & \\cdots & w_{k,k}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, similar to the one dimensional case,  $k$ is the window size and indicates the size of the filter applied to the input matrix $\\mathbf{X}$. From the one dimensional case we can extend the notion of a sub vector to a sub matrix. Let $\\mathbf{X}_k(i,j)$ denote the $k \\times k$ submatrix of $\\mathbf{X}$ starting at row $i$ and column $j$ as\n",
    "\n",
    "$$\\mathbf{X}_k(i,j) = \\begin{bmatrix}\n",
    "x_{i,j} & x_{i,~(j+1)} & \\cdots & x_{i,~(j+k-1)} \\\\\n",
    "x_{(i+1),~j} & x_{(i+2),~j} & \\cdots & x_{(i+1), (j+k-1)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1), ~j} & x_{(i+1),(j+1)} & \\cdots & x_{(i+k-1),(j+k-1)}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where for two indices, give that this is a square matrix, the range is simple $1 \\leq (i,j) \\leq n-k+1$.\n",
    "\n",
    "AS for the one dimensional case, to simplify the notation, we adopt the convention that for a matrix $\\mathbf{A} \\in \\mathbb{R}^{k \\times k}$ define the summation operator as one that adds all elements of the matrix.\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{A}) = \\sum_{i=1}^{k}\\sum_{j=1}^{k} a_{i,j}$$\n",
    "\n",
    "Then, we can define  a general two dimensional convolution operation between matrices $\\mathbf{X}$ and $\\mathbf{W}$, as\n",
    "\n",
    "\n",
    "$$\\mathbf{X} \\ast \\mathbf{W} = \\begin{bmatrix}\n",
    "\\text{Sum}(x_k(1,1) \\odot \\mathbf{W}) &\\text{Sum}(x_k(1,2) \\odot \\mathbf{W}) & \\cdots & \\text{Sum}(x_k(1,n-k+1) \\odot \\mathbf{W}) \\\\\n",
    "\\text{Sum}(x_k(2,1) \\odot \\mathbf{W}) & \\text{Sum}(x_k(2,2) \\odot \\mathbf{W}) & \\cdots &\\text{Sum}(x_k(2,n-k+1) \\odot \\mathbf{W})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Sum}(x_k(n-k+1,1) \\odot \\mathbf{W}) & \\text{Sum}(x_k(n-k+1,2) \\odot \\mathbf{W}) & \\cdots & \\text{Sum}(x_k(n-k+1,n-k+1) \\odot \\mathbf{W})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_k(i,j) \\odot \\mathbf{W})=\\sum_{a=1}^{k}\\sum_{b=1}^{k} x_{(i+a-1),(j+b-1)} w_{a,b}$$\n",
    "\n",
    "The convolution of $\\mathbf{X} \\in \\mathbf{R}^{n \\times n}$ and $\\mathbf{W} \\in \\mathbf{R}^{k \\times k}$ results in a $(n-k+1) \\times (n-k+1)$ matrix.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's consider a particular example with input matrix $\\mathbf{X}$ with dimension  $3 \\times 3$ (n = 3) and a weight matrix with dimension  $2 \\times 2$ (k = 2). The matrices are illustrated in the following figure:\n",
    "\n",
    "<center><img src = \"figures/2D-conv.png\" width=\"800\" height=\"400\"/></center>\n",
    "\n",
    "The convolution steps for the sliding windows of $\\mathbf{X}$ with the filter $\\mathbf{W}$ illustrated in the figure are mathematically translated to:\n",
    "\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_k(1,1) \\odot \\mathbf{W})=\\text{Sum}\\bigg(\n",
    "    \\begin{bmatrix} \n",
    "    1 & 2 \\\\\n",
    "    3 & 1 \n",
    "    \\end{bmatrix} \n",
    "    \\odot \n",
    "    \\begin{bmatrix} \n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "    \\end{bmatrix} \\bigg) =  2$$\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_2(1,2) \\odot \\mathbf{W}) = \\text{Sum}\\bigg(\n",
    "    \\begin{bmatrix} \n",
    "    2 & 2 \\\\\n",
    "    1 & 4\n",
    "    \\end{bmatrix} \n",
    "    \\odot\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "    \\end{bmatrix} \\bigg)= 6$$\n",
    "\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_2(2,1) \\odot \\mathbf{W}) = \\text{Sum}\\bigg( \n",
    "    \\begin{bmatrix} \n",
    "    3 & 1 \\\\ \n",
    "    2 & 1 \\end{bmatrix} \n",
    "    \\odot \\begin{bmatrix} \n",
    "    1 & 0 \\\\ \n",
    "    0 & 1 \n",
    "    \\end{bmatrix} \\bigg)= 4$$\n",
    "\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_2(2,2) \\odot \\mathbf{W}) = \\text{Sum}\\bigg( \n",
    "    \\begin{bmatrix} \n",
    "    1 & 4 \\\\\n",
    "    3 & 3 \\end{bmatrix}\n",
    "    \\odot\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 \\\\ \n",
    "    0 & 1\n",
    "    \\end{bmatrix} \\bigg) = 4$$\n",
    "\n",
    "The convolution $\\mathbf{X}*\\mathbf{W}$ has size $2 \\times 2$, since  $n - k + 1 = 3 - 2 + 1 = 2$, and is given by\n",
    "\n",
    "$$\\mathbf{X}*\\mathbf{W} = \n",
    "    \\begin{bmatrix} \n",
    "        \\text{Sum}(\\mathbf{X}_2(1,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{X}_2(1,2) \\odot \\mathbf{W}) \\\\\n",
    "        \\\\\n",
    "        \\text{Sum}(\\mathbf{X}_2(2,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{X}_2(2,2) \\odot \\mathbf{W}) \n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix} \n",
    "        2 & 6 \\\\\n",
    "        4 & 4 \n",
    "    \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Three dimensional Convolution on CNNs**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extend the convolution operation to a three-dimensional matrix, also called a rank-3 tensor. In the context of convolutional neural networks, the first dimension comprises the depth (or the number of 2D slices stacked along the depth axis), the second the rows (height), and the third the columns (width). A rank-3 tensor in tthe context of a CNN would be a single channel of a 3D volume. Here's how we can represent a single-channel 3D tensor mathematically:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{X}=\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,1} & x_{1,2,1} & \\cdots & x_{1,n,1} \\\\\n",
    "x_{2,1,1} & x_{2,2,1} & \\cdots & x_{2,n,1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1,1} & x_{n,2,1} & \\cdots & x_{n,n,1}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,2} & x_{1,2,2} & \\cdots & x_{1,n,2} \\\\\n",
    "x_{2,1,2} & x_{2,2,2} & \\cdots & x_{2,n,2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1,2} & x_{n,2,2} & \\cdots & x_{n,n,2}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\vdots\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,m} & x_{1,2,m} & \\cdots & x_{1,n,m} \\\\\n",
    "x_{2,1,m} & x_{2,2,m} & \\cdots & x_{2,n,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1,m} & x_{n,2,m} & \\cdots & x_{n,n,m}\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix},~~\n",
    "\n",
    "\n",
    "\\mathbf{W}= \\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{1,1,1} & w_{1,2,1} & \\cdots & w_{1,k,1} \\\\\n",
    "w_{2,1,1} & w_{2,2,1} & \\cdots & w_{2,k,1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{k,1,1} & w_{k,2,1} & \\cdots & w_{k,k,1}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "w_{1,1,2} & w_{1,2,2} & \\cdots & w_{1,k,2} \\\\\n",
    "w_{2,1,2} & w_{2,2,2} & \\cdots & w_{2,k,2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{k,1,2} & w_{k,2,2} & \\cdots & w_{k,k,2}\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\vdots\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "w_{1,1,r} & w_{1,2,r} & \\cdots & w_{1,k,r} \\\\\n",
    "w_{2,1,r} & w_{2,2,r} & \\cdots & w_{2,k,r} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{k,1,r} & w_{k,2,r} & \\cdots & w_{k,k,r}\n",
    "\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similar to convolutions in other dimensions, the window size must satisfy $k \\leq n$, and the total number of slice matrices along the depth of the filter tensor should not exceed the depth of the input tensor, meaning $ r \\leq m $. Although this mathematical representation of a tensor may seem complex at first, it closely resembles how Python libraries like NumPy represent a tensor.\n",
    "\n",
    "Let's extend the idea of defining a sub-tensor from the input tensor $\\mathbf{X}$. Let $\\mathbf{X}_k(i,j, q)$ denote a $ k \\times k \\times r $ sub-tensor of $\\mathbf{X}$ that starts at row $i$, column $j$, and depth $q$ of $\\mathbf{X}$. It is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_k(i,j,q)=\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,q} & x_{i,(j+1),q} & \\cdots & x_{i,(j+k-1),q} \\\\\n",
    "x_{(i+1),j,q} & x_{(i+1),(j+1),q} & \\cdots & x_{(i+1),(j+k-1),q} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,q} & x_{(i+k-1),(j+1),q} & \\cdots & x_{(i+k-1),(j+k-1),q}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,(q+1)} & x_{i,(j+1),(q+1)} & \\cdots & x_{i,(j+k-1),(q+1)} \\\\\n",
    "x_{(i+1),j,(q+1)} & x_{(i+1),(j+1),(q+1)} & \\cdots & x_{(i+1),(j+k-1),(q+1)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,(q+1)} & x_{(i+k-1),(j+1),(q+1)} & \\cdots & x_{(i+k-1),(j+k-1),(q+1)}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\vdots\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,(q+r-1)} & x_{i,(j+1),(q+r-1)} & \\cdots & x_{i,(j+k-1),(q+r-1)} \\\\\n",
    "x_{(i+1),j,(q+r-1)} & x_{(i+1),(j+1),(q+r-1)} & \\cdots & x_{2,n,(q+r-1)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,(q+r-1)} & x_{(i+k-1),(j+1),(q+r-1)} & \\cdots & x_{(i+k-1),(j+k-1),(q+r-1)}\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where we have for rows and columns the same range $1 \\leq (i,j) \\leq n-k+1$ as in two dimension and the addition of the channels range $1 \\leq q \\leq m-r+1$.\n",
    "\n",
    "Typically in CNNs, we use a 3D filter $\\mathbf{W} \\in \\mathbb{R}^{k \\times k \\times m}$, with the number of channels $r=m$, the same as the number of channels of the input tensor $\\mathbf{X} \\in \\mathbb{R}^{n \\times n \\times m}$. This is a simplification, where we would have $q = 1$ and the sub tensor as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_k(i,j,q)=\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,1} & x_{i,(j+1),1} & \\cdots & x_{i,(j+k-1),1} \\\\\n",
    "x_{(i+1),j,1} & x_{(i+1),(j+1),1} & \\cdots & x_{(i+1),(j+k-1),1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,1} & x_{(i+k-1),(j+1),1} & \\cdots & x_{(i+k-1),(j+k-1),1}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,2} & x_{i,(j+1),2} & \\cdots & x_{i,(j+k-1),2} \\\\\n",
    "x_{(i+1),j,2} & x_{(i+1),(j+1),2} & \\cdots & x_{(i+1),(j+k-1),2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,2} & x_{(i+k-1),(j+1),2} & \\cdots & x_{(i+k-1),(j+k-1),2}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\vdots\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,m} & x_{i,(j+1),m} & \\cdots & x_{i,(j+k-1),m} \\\\\n",
    "x_{(i+1),j,m} & x_{(i+1),(j+1),m} & \\cdots & x_{2,n,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,m} & x_{(i+k-1),(j+1),m} & \\cdots & x_{(i+k-1),(j+k-1),m}\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "This way we fixed the value of the channels to be equal between filter and input. \n",
    "\n",
    "As for others dimensions, given a tensor $\\mathbf{A} \\in \\mathbb{R}^{k \\times k \\times m}$, we define the summation operator as one that adds all elements of the tensor. That is, \n",
    "\n",
    "$$\\text{Sum}(\\mathbf{A}) = \\sum_{a=1}^{k}\\sum_{b=1}^{k}\\sum_{q=1}^{m}a_{ijq}$$\n",
    "\n",
    "Before generalizing the convolution operation for three dimension, let's consider a example to illustrate the logic behind all this dense mathematical notation.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Consider a input tensor $\\mathbf{X}$ with dimension  $3 \\times 3 \\times 3$ (n = 3 and m = 3) and a filter with dimension  $2 \\times 2 \\times 3$ ( windows size with k = 2 and r = m = 3). The tensors are illustrated in the following figure:\n",
    "\n",
    "<center><img src = \"figures/3dd-conv.png\" ></center>\n",
    "\n",
    "\n",
    "The convolution steps for the sliding windows of $\\mathbf{x}$ with the filter $\\mathbf{w}$ illustrated in the figure are:\n",
    "\n",
    "$$\n",
    "\\text{Sum}(\\mathbf{X}_2(1,1,1) \\odot \\mathbf{W}) = \\text{Sum}\\bigg( \n",
    "\\begin{bmatrix} \n",
    "    \\begin{bmatrix} \n",
    "        1 & -1 \\\\ \n",
    "        2 & 1 \n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        2 & 1 \\\\\n",
    "        3 & -1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        1 & -2 \\\\ \n",
    "        2 & 1 \n",
    "    \\end{bmatrix} \n",
    "\\end{bmatrix} \n",
    "\\odot \n",
    "\\begin{bmatrix} \n",
    "    \\begin{bmatrix} \n",
    "        1 & 1 \\\\\n",
    "        2 & 0 \n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        1 & 0 \\\\ \n",
    "        0 & 1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 \\\\\n",
    "        1 & 0 \n",
    "    \\end{bmatrix} \n",
    "\\end{bmatrix} \\bigg) =  \\text{Sum}\\bigg(   \n",
    "    \\begin{bmatrix} \n",
    "    \\begin{bmatrix} \n",
    "        1  & -1  \\\\ \n",
    "        4 & 0 \n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        2 & 0 \\\\\n",
    "        0 & -1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        0 & -2 \\\\ \n",
    "        2 & 0 \n",
    "    \\end{bmatrix} \n",
    "\\end{bmatrix} \\bigg) = 1 - 1 + 4 +2 -1 -2+ 2 = 5 \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Sum}(\\mathbf{X}_2(1,2,1) \\odot \\mathbf{W}) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        -1 & 3 \\\\\n",
    "        1 & -4\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        1 & 3 \\\\\n",
    "        -1 & 1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        -2 & 4 \\\\\n",
    "        1 & -2\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        1 & 1 \\\\\n",
    "        2 & 0\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 \\\\\n",
    "        0 & 1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 \\\\\n",
    "        1 & 0\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    -1 & 3 \\\\\n",
    "    2 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 4 \\\\\n",
    "    1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = -1 + 3 + 2 + 1 + 1 + 4 + 1 = 11 \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Sum}(\\mathbf{X}_2(2,1,1) \\odot \\mathbf{W}) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    2 & 1 \\\\\n",
    "    3 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    3 & -1 \\\\\n",
    "    1 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    2 & 1 \\\\\n",
    "    1 & 3\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 1 \\\\\n",
    "    2 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 1 \\\\\n",
    "    1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    2 & 1 \\\\\n",
    "    6 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    3 & 0 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 1 \\\\\n",
    "    1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = 2 + 1 + 6 + 3 + 1 + 1 + 1 = 15\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Sum}(\\mathbf{X}_2(2,2,1) \\odot \\mathbf{W}) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 4 \\\\\n",
    "    1 & 2\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    -1 & 1 \\\\\n",
    "    1 & -2\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    1 & -2 \\\\\n",
    "    3 & -1\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 1 \\\\\n",
    "    2 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 1 \\\\\n",
    "    1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 4 \\\\\n",
    "    2 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    -1 & 0 \\\\\n",
    "    0 & -2\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & -2 \\\\\n",
    "    3 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = 1 + 4 +  2 + -1 - 2 - 2 + 3  = 5 \n",
    "$$\n",
    "\n",
    "The convolution $\\mathbf{X}*\\mathbf{W}$ has size $2 \\times 2$, since  $n - k + 1 = 3 - 2 + 1 = 2$, and $r =m = 3$; it is is given as\n",
    "\n",
    " $$\\mathbf{X}*\\mathbf{W} = \n",
    " \\begin{bmatrix}\n",
    "    \\text{Sum}(\\mathbf{X}_2(1,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{X}_2(1,2) \\odot \\mathbf{W}) \\\\\n",
    "    \\\\\n",
    "    \\text{Sum}(\\mathbf{X}_2(2,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{X}_2(2,2) \\odot \\mathbf{W})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    5 & 11 \\\\\n",
    "    15 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that because we fixed the third dimension as $r = m$, by restricting the input tensor $\\mathbf{X} \\in \\mathbb{R}^{n \\times n \\times m}$ and the filter tensor $\\mathbf{W} \\in \\mathbb{R}^{k \\times k \\times m}$ to have the same number of channels, we get a $(n-k+1) \\times (n-k+1) \\times 1$ matrix instead of a tensor, where the last dimension can be dropped, since there is no freedom to slide the filter in the third dimension. Because of this, the notation for the sub tensor can be rewrite as $\\text{Sum}(\\mathbf{X}_k(i,j,q) \\odot \\mathbf{W}) = \\text{Sum}(\\mathbf{X}_k(i,j) \\odot \\mathbf{W})$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor X:\n",
      " (1, 3, 3, 3, 1)\n",
      "Shape of filter tensor W:\n",
      " (3, 2, 2, 1, 1)\n",
      "Convolved output shape with channel and batch dimension:\n",
      " (1, 1, 2, 2, 1)\n",
      "Convolved output:\n",
      " [[ 5. 11.]\n",
      " [15.  5.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [[1, -1, 3],\n",
    "     [2, 1, 4],\n",
    "     [3, 1, 2]],\n",
    "\n",
    "    [[2, 1, 3],\n",
    "     [3, -1, 1],\n",
    "     [1, 1, -2]],\n",
    "\n",
    "    [[1, -2, 4],\n",
    "     [2, 1, -2],\n",
    "     [1, 3, -1]]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# TensorFlow expects the input to have a shape of [batch, (depth, height, width), channels]\n",
    "# Add a batch dimension and a channel dimension to X\n",
    "X = X.reshape(1, *X.shape, 1) \n",
    "# create a simple 3D kernel\n",
    "W = np.array([\n",
    "    [[1, 1],\n",
    "     [2, 0]],\n",
    "\n",
    "    [[1, 0],\n",
    "     [0, 1]],\n",
    "\n",
    "    [[0, 1],\n",
    "     [1, 0]]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# TensorFlow expects the filter to have a shape of [(depth, height, width), in_channels = 1, out_channels = 1]\n",
    "# Since our input has a single channel (in_channels = 1) and we want a single output channel ( out_channels = 1),\n",
    "# Add those dimensions to W\n",
    "W = W.reshape(*W.shape, 1, 1) \n",
    "\n",
    "# 3D convolution\n",
    "output = tf.nn.conv3d(input=X, filters=W, strides=[1, 1, 1, 1, 1], padding=\"VALID\")\n",
    "\n",
    "# squeeze to remove the redundant dimensions of batch and channel\n",
    "output_2d = output.numpy().squeeze()    \n",
    "\n",
    "print(\"Shape of input tensor X:\\n\", X.shape)\n",
    "print(\"Shape of filter tensor W:\\n\", W.shape)\n",
    "print(\"Convolved output shape with channel and batch dimension:\\n\", output.shape)\n",
    "print(\"Convolved output:\\n\", output_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Typically in CNNs, we use a 3D filter $\\mathbf{W} \\in \\mathbb{R}^{k \\times k \\times m}$, with the number of channels $r=m$, the same as the number of channels of the input tensor $\\mathbf{X} \\in \\mathbb{R}^{n \\times n \\times m}$.\n",
    "\n",
    "Each channel of the input tensor $\\mathbf{X}$ represents a different aspect (or feature) of the input tensor, and **the filter is used to extract those features from the input tensor $\\mathbf{X}$ by convolving it with the filter tensor $\\mathbf{W}$**. By using a filter $\\mathbf{W}$ with the same number of channels as the input signal $\\mathbf{X}$, we can ensure that the filter is applied to every channel of the input data. This allows the network to learn features from each channel of the input signal separately, which can improve the overall performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Each channel of the input signal $\\mathbf{X}$ represents a different aspect or feature of the input data, and **the filter is used to extract features from the input data $\\mathbf{X}$ by convolving it with the filter tensor $\\mathbf{W}$**. By using a filter $\\mathbf{W}$ with the same number of channels as the input signal $\\mathbf{X}$, we can ensure that the filter is applied to every channel of the input data. This allows the network to learn features from each channel of the input signal separately, which can improve the overall performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If  $r \\leq m - r + 1$, then the resulting convolution would have dimension $(n-k+1) \\times (n-k+1) \\times (m-k+1)$ instead of $(n-k+1) \\times (n-k+1) \\times 1$. We will see that this simplification is important to create a channel for each convolution between a tensor $\\mathbf{X}$ with more then one filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
